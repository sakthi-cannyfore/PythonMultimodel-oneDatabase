import express from "express";
import cors from "cors";
import dotenv from "dotenv";
import duckdb from "duckdb";
import { pipeline } from "@xenova/transformers";
import fetch from "node-fetch";

dotenv.config();

const app = express();
app.use(cors());
app.use(express.json());

const DB_PATH =
  process.env.DB_PATH ||
  "C:/Users/Sakthi/Desktop/Shadow/Sowi/python/rag_db.duckdb"; 
const db = new duckdb.Database(DB_PATH);

console.log("🧠 Loading local embedding model (all-MiniLM-L6-v2)...");
const embedder = await pipeline(
  "feature-extraction",
  "Xenova/all-MiniLM-L6-v2"
);
console.log(" Model loaded successfully!");

function cosineSimilarity(vecA, vecB) {
  const dot = vecA.reduce((acc, val, i) => acc + val * vecB[i], 0);
  const magA = Math.sqrt(vecA.reduce((acc, val) => acc + val * val, 0));
  const magB = Math.sqrt(vecB.reduce((acc, val) => acc + val * val, 0));
  return dot / (magA * magB);
}

async function getEmbedding(text) {
  const output = await embedder(text, { pooling: "mean", normalize: true });
  return Array.from(output.data);
}

async function retrieveTopMatches(questionEmbedding, topK = 3) {
  return new Promise((resolve, reject) => {
    db.all("SELECT chunk, embedding FROM chunks", (err, rows) => {
      if (err) return reject(err);
      if (!rows?.length)
        return reject(new Error("No embeddings found in database."));

      const scored = rows.map((row) => {
        const storedEmbedding = Array.from(
          new Float32Array(Buffer.from(row.embedding.buffer))
        );
        const score = cosineSimilarity(questionEmbedding, storedEmbedding);
        return { chunk: row.chunk, score };
      });

      const top = scored.sort((a, b) => b.score - a.score).slice(0, topK);
      resolve(top);
    });
  });
}

app.post("/ask", async (req, res) => {
  const { question } = req.body;
  if (!question) return res.status(400).json({ error: "Missing 'question'." });

  try {
    console.log(" Received question:", question);

    const questionEmbedding = await getEmbedding(question);

    const topMatches = await retrieveTopMatches(questionEmbedding, 3);
    const contextText = topMatches.map((m) => m.chunk).join("\n---\n");

    const prompt = `You are a helpful assistant.
Use the following context to answer accurately and concisely.

Context:
${contextText}

Question: ${question}`;

    const llmResponse = await fetch(
      "https://openrouter.ai/api/v1/chat/completions",
      {
        method: "POST",
        headers: {
          Authorization: `Bearer ${process.env.OPENROUTER_API_KEY}`,
          "Content-Type": "application/json",
        },
        body: JSON.stringify({
          model: "google/gemini-2.5-flash",
          messages: [{ role: "user", content: prompt }],
        }),
      }
    );

    const llmData = await llmResponse.json();
    const reply =
      llmData?.choices?.[0]?.message?.content ||
      "No response generated by LLM.";

    res.json({ reply, topMatches });
  } catch (error) {
    console.error("❌ RAG Error:", error);
    res.status(500).json({ error: error.message || "Server error" });
  }
});

const PORT = 5000;
app.listen(PORT, () =>
  console.log(`🚀 Local RAG Server running on http://localhost:${PORT}`)
);
